{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTH3302 - M√©thodes probabilistes et statistiques pour I.A.\n",
    "#### Polytechnique Montr√©al\n",
    "\n",
    "\n",
    "### Projet A2024\n",
    "\n",
    "-----\n",
    "\n",
    "# Pr√©diction de la consommation en carburant de voitures r√©centes.\n",
    "\n",
    "### Contexte\n",
    "\n",
    "## TODO\n",
    "\n",
    "### Objectif\n",
    "\n",
    "## TODO\n",
    "\n",
    "### Donn√©es\n",
    "Les donn√©es utilis√©es pour inf√©rer la consommation de carburant sont les suivantes :\n",
    "\n",
    "## TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"annee\";\"type\";\"nombre_cylindres\";\"cylindree\";\"transmission\";\"boite\";\n",
    "\n",
    "#### pistes:\n",
    "\n",
    "preprocessing:\n",
    "\n",
    "quoi faire avec les variables fortement corr√©lees, cylindree et nombre cylindres (Supprimer une des deux, soit celle qui a le moins d'impact sur la consommation en carburant, ou les combiner en une seule variable) (√âTAPE 1)\n",
    "(si par exemple, on voit une augmentation disproportionn√©e de la consommation en carburant avec la cylindr√©e, on pourrait penser √† les combiner en une seule variable)\n",
    "Si nombre_cylindres est une quantit√© discr√®te et cylindree est une mesure continue (en litres), leur produit peut √™tre vu comme une \"capacit√© moteur totale\", une m√©trique significative pour des mod√®les pr√©dictifs.\n",
    "\n",
    "nouvelle variable comme age du vehicule (2024 - year) (ca reduit l'importance de l'ann√©e dans les donn√©es) (Comparer avec juste l'enlever pour voir si ca ameliore le modele) (√âTAPE 2)\n",
    "\n",
    "reperer les outliers et les traiter\n",
    "\n",
    "equilibrage des classes (sur ou sous representation des types de vehicules)\n",
    "\n",
    "zscore normalization sur cylindree (ou nombre cylindres selon chat gpt mais pas certain)\n",
    "\n",
    "ordinal encoding\n",
    "one hot encoding (si peu de cat√©gories) : Cr√©ez une colonne pour chaque cat√©gorie.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV, DataFrames, Statistics, Dates, Gadfly, Combinatorics, Plots, StatsBase, StatsPlots, Random, StatsModels, GLM, LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = CSV.read(\"../data/raw/train.csv\", DataFrame; delim=\";\")\n",
    "test =  CSV.read(\"../data/raw/test.csv\", DataFrame; delim=\";\") #ne contient pas la varialbe consommation\n",
    "\n",
    "Random.seed!(1234) #pour la reproductibilit\n",
    "\n",
    "ntrain = round(Int, .8*nrow(full_train)) #80% des donn√©es pour l'entrainement: 80% * nb de lignes\n",
    "\n",
    "train_id = sample(1:nrow(full_train), ntrain, replace=false, ordered=true) #√©chantillonnage al√©atoire pour l'entrainement\n",
    "valid_id = setdiff(1:nrow(full_train), train_id) #√©chantillon de validation. prend celles qui ne sont pas dans l'√©chantillon d'entrainement\n",
    "\n",
    "train = full_train[train_id, :]  \n",
    "valid = full_train[valid_id, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. √âtude des donn√©es "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stats = describe(train)\n",
    "testing_stats = describe(test)\n",
    "print(\"Training Set: \\n\", training_stats)\n",
    "print(\"\\n Testing Set: \\n\", testing_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploration des donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function safe_parse_float(x)\n",
    "    try\n",
    "        return parse(Float64, x)\n",
    "    catch\n",
    "        return missing\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function one_hot_encode(df, cols, levels_dict)\n",
    "    for col in cols\n",
    "        levels_col = levels_dict[col]\n",
    "        for level in levels_col\n",
    "            new_col = Symbol(string(col) * \"_\" * string(level))\n",
    "            df[!, new_col] = ifelse.(df[!, col] .== level, 1.0, 0.0)\n",
    "        end\n",
    "        # Remove the original column\n",
    "        select!(df, Not(col))\n",
    "    end\n",
    "    return df\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Analyse des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = deepcopy(train)\n",
    "data = dropmissing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© des donn√©es\n",
    "println(describe(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corr√©lation entre les variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [:annee, :nombre_cylindres, :cylindree, :consommation]\n",
    "\n",
    "M = cor(Matrix(data[:, numeric_cols]))\n",
    "\n",
    "# Afficher la matrice de corr√©lation\n",
    "println(\"Matrice de corr√©lation :\")\n",
    "println(M)\n",
    "\n",
    "# PLOT\n",
    "(n,m) = size(M)\n",
    "heatmap(M, fc=cgrad([:white,:dodgerblue4]), xticks=(1:m,numeric_cols), xrot=90, yticks=(1:m,numeric_cols), yflip=true)\n",
    "annotate!([(j, i, text(round(M[i,j],digits=3), 8,\"Computer Modern\",:black)) for i in 1:n for j in 1:m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `nombre_cylindres` et `cylindree` est tr√®s √©lev√©e, ce qui indique une forte relation positive. Cela sugg√®re que le nombre de cylindres est fortement associ√© √† la cylindr√©e des v√©hicules.\n",
    "\n",
    "2. La corr√©lation entre `cylindree` et `consommation` est √©galement √©lev√©e, montrant qu'une augmentation de la cylindr√©e est associ√©e √† une augmentation de la consommation (par exemple, les moteurs plus gros consomment plus de carburant).\n",
    "\n",
    "3. Une corr√©lation similaire existe entre `nombre_cylindres` et `consommation`, ce qui est logique, car le nombre de cylindres et la cylindr√©e sont li√©s.\n",
    "\n",
    "4. Les corr√©lations entre annee et les autres variables sont faibles et n√©gatives, indiquant que les variables comme le nombre de cylindres, la cylindr√©e et la consommation ont l√©g√®rement diminu√© avec le temps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consommation par type de v√©hicule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "Gadfly.plot(train, x=:type, y=:consommation, Geom.boxplot )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_categories = unique(skipmissing(data[:, :type]))\n",
    "occurences = [sum(skipmissing(data[:, :type]) .== category) for category in unique_categories]\n",
    "occurences = DataFrame(category = unique_categories, occurences = occurences)\n",
    "occurences = occurences[occurences.occurences .> 10, :] #TODO INVESTIGATE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consommation en fonction du type v√©hicule moyen :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "vehicule_moyenne = filter(row -> row.type == \"voiture_moyenne\", data)\n",
    "Gadfly.plot(vehicule_moyenne, x=:annee, y=:consommation, color=:type, Geom.point, Geom.smooth(method=:loess), Guide.xlabel(\"Ann√©e\"), Guide.ylabel(\"Consommation (L/100km)\"), Guide.colorkey(\"Type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consommation en fonction du type VUS_petit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "vehicule_VUSp = filter(row -> row.type == \"VUS_petit\", data)\n",
    "Gadfly.plot(vehicule_VUSp, x=:annee, y=:consommation, color=:type, Geom.point, Geom.smooth(method=:loess), Guide.xlabel(\"Ann√©e\"), Guide.ylabel(\"Consommation (L/100km)\"), Guide.colorkey(\"Type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consommation en fonction du type v√©hicule compacte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "voiture_compacte = filter(row -> row.type == \"voiture_compacte\", data)\n",
    "Gadfly.plot(voiture_compacte, x=:annee, y=:consommation, color=:type, Geom.point, Geom.smooth(method=:loess), Guide.xlabel(\"Ann√©e\"), Guide.ylabel(\"Consommation (L/100km)\"), Guide.colorkey(\"Type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consommation en fonction du type v√©hicule 2 places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "voiture_deux_places = filter(row -> row.type == \"voiture_deux_places\", data)\n",
    "Gadfly.plot(voiture_deux_places, x=:annee, y=:consommation, color=:type, Geom.point, Geom.smooth(method=:loess), Guide.xlabel(\"Ann√©e\"), Guide.ylabel(\"Consommation (L/100km)\"), Guide.colorkey(\"Type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consommation en fonction du type v√©hicule camionnette standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "camionnette_standard = filter(row -> row.type == \"camionnette_standard\", data)\n",
    "Gadfly.plot(camionnette_standard, x=:annee, y=:consommation, color=:type, Geom.point, Geom.smooth(method=:loess), Guide.xlabel(\"Ann√©e\"), Guide.ylabel(\"Consommation (L/100km)\"), Guide.colorkey(\"Type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consommation en fonction du type v√©hicule mini compacte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "voiture_minicompacte = filter(row -> row.type == \"voiture_minicompacte\", data)\n",
    "Gadfly.plot(voiture_minicompacte, x=:annee, y=:consommation, color=:type, Geom.point, Geom.smooth(method=:loess), Guide.xlabel(\"Ann√©e\"), Guide.ylabel(\"Consommation (L/100km)\"), Guide.colorkey(\"Type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consommation en fonction du type v√©hicule VUS standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "VUS_standard = filter(row -> row.type == \"VUS_standard\", data)\n",
    "Gadfly.plot(VUS_standard, x=:annee, y=:consommation, color=:type, Geom.point, Geom.smooth(method=:loess), Guide.xlabel(\"Ann√©e\"), Guide.ylabel(\"Consommation (L/100km)\"), Guide.colorkey(\"Type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consommation en fonction du type v√©hicule sous-compacte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "voiture_sous_compacte = filter(row -> row.type == \"voiture_sous_compacte\", data)\n",
    "Gadfly.plot(voiture_sous_compacte, x=:annee, y=:consommation, color=:type, Geom.point, Geom.smooth(method=:loess), Guide.xlabel(\"Ann√©e\"), Guide.ylabel(\"Consommation (L/100km)\"), Guide.colorkey(\"Type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consommation par cylindr√©e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consommation par nombre de cylindres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consommation par ann√©e //TODO METTRE UNE NOTE COMME QUOI PAS BESOIN D'INVESTIGUER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. R√©gression lin√©aire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random.seed!(1234) #pour la reproductibilit√©\n",
    "\n",
    "# ntrain = round(Int, .8*nrow(full_train)) #80% des donn√©es pour l'entrainement: 80% * nb de lignes\n",
    "\n",
    "# train_id = sample(1:nrow(full_train), ntrain, replace=false, ordered=true) #√©chantillonnage al√©atoire pour l'entrainement\n",
    "# valid_id = setdiff(1:nrow(full_train), train_id) #√©chantillon de validation. prend celles qui ne sont pas dans l'√©chantillon d'entrainement\n",
    "\n",
    "# train = full_train[train_id, :]  \n",
    "# valid = full_train[valid_id, :]\n",
    "\n",
    "# first(train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capacite_moteur = :nombre_cylindres * :cylindree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert annee column into age\n",
    "train.age = 2024 .- train.annee\n",
    "valid.age = 2024 .- valid.annee\n",
    "test.age = 2024 .- test.annee\n",
    "\n",
    "train = select!(train, Not(:annee))\n",
    "valid = select!(valid, Not(:annee))\n",
    "test = select!(test, Not(:annee))\n",
    "\n",
    "## drop missing values\n",
    "train = dropmissing(train)\n",
    "valid = dropmissing(valid)\n",
    "test = dropmissing(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets that contain 'consommation'\n",
    "datasets_with_consommation = [train, valid]\n",
    "\n",
    "# Datasets without 'consommation'\n",
    "datasets_without_consommation = [test]\n",
    "\n",
    "# Apply replacements to 'cylindree' in all datasets\n",
    "for df in [train, valid, test]\n",
    "    df.cylindree = replace.(df.cylindree, \",\" => \".\")\n",
    "end\n",
    "\n",
    "# Apply replacements to 'consommation' only in datasets that have it\n",
    "for df in datasets_with_consommation\n",
    "    df.consommation = replace.(df.consommation, \",\" => \".\")\n",
    "end\n",
    "\n",
    "# Convert 'cylindree' to float in all datasets\n",
    "for df in [train, valid, test]\n",
    "    df.cylindree = safe_parse_float.(df.cylindree)\n",
    "end\n",
    "\n",
    "# Convert 'consommation' to float in datasets with 'consommation'\n",
    "for df in datasets_with_consommation\n",
    "    df.consommation = safe_parse_float.(df.consommation)\n",
    "end\n",
    "\n",
    "# Drop missing values in all datasets\n",
    "for df in [train, valid, test]\n",
    "    dropmissing!(df)\n",
    "end\n",
    "\n",
    "# # Encode 'boite' column in all datasets\n",
    "for df in [train, valid, test]\n",
    "    df.boite = ifelse.(df.boite .== \"automatique\", 1.0, 0.0)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical columns\n",
    "categorical_cols = [:type, :transmission]\n",
    "\n",
    "# Collect unique levels from the training set\n",
    "levels_dict = Dict()\n",
    "for col in categorical_cols\n",
    "    levels_dict[col] = unique(train[!, col])\n",
    "end\n",
    "\n",
    "train = one_hot_encode(train, categorical_cols, levels_dict)\n",
    "valid = one_hot_encode(valid, categorical_cols, levels_dict)\n",
    "test = one_hot_encode(test, categorical_cols, levels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "function normalize(df, cols)\n",
    "    for col in cols\n",
    "        df[!, col] = (df[!, col] .- mean(df[!, col])) ./ std(df[!, col])\n",
    "    end\n",
    "    return df\n",
    "end\n",
    "\n",
    "# cols_to_normalize = [:cylindree, :age, :nombre_cylindres]\n",
    "# train = normalize(train, cols_to_normalize)\n",
    "# valid = normalize(valid, cols_to_normalize)\n",
    "# test = normalize(test, cols_to_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GLM.lm(@formula(consommation ~ age + transmission_integrale + transmission_propulsion + transmission_traction + transmission_4x4 + cylindree), train)\n",
    "# Prediction avec l'ensemble de validation\n",
    "valid_prediction = GLM.predict(model, valid)\n",
    "# Trouver la moyenne de prediction\n",
    "mean_prediction = mean(valid_prediction)\n",
    "# Remplacer les missing par la moyenne\n",
    "valid_prediction = coalesce.(valid_prediction, mean_prediction)\n",
    "# Transformer les predictions en valeur entiere\n",
    "#v = Int.(round.(valid_prediction, digits=0)) #mettre une commentaire sur la difference que ca entraine sur le rmse\n",
    "# Calculer le RMSE\n",
    "rmse_valid = sqrt(mean((valid_prediction - valid.consommation).^2))\n",
    "println(\"RMSE: \", rmse_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = nrow(test)\n",
    "\n",
    "id = 1:n\n",
    "\n",
    "yÃÇ = GLM.predict(model, test)\n",
    "\n",
    "df_pred = DataFrame(id=id, consommation=yÃÇ)\n",
    "\n",
    "name = \"linear/\" * string(rmse_valid) * \".csv\"\n",
    "CSV.write(\"../submissions/\" * name, df_pred)\n",
    "println(\"Predictions exported successfully to \" * name*\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. R√©gression bayesienne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert annee column into age\n",
    "train.age = 2024 .- train.annee\n",
    "valid.age = 2024 .- valid.annee\n",
    "test.age = 2024 .- test.annee\n",
    "\n",
    "train = select!(train, Not(:annee))\n",
    "valid = select!(valid, Not(:annee))\n",
    "test = select!(test, Not(:annee))\n",
    "\n",
    "## drop missing values\n",
    "train = dropmissing(train)\n",
    "valid = dropmissing(valid)\n",
    "test = dropmissing(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets that contain 'consommation'\n",
    "datasets_with_consommation = [train, valid]\n",
    "\n",
    "# Datasets without 'consommation'\n",
    "datasets_without_consommation = [test]\n",
    "\n",
    "# Apply replacements to 'cylindree' in all datasets\n",
    "for df in [train, valid, test]\n",
    "    df.cylindree = replace.(df.cylindree, \",\" => \".\")\n",
    "end\n",
    "\n",
    "# Apply replacements to 'consommation' only in datasets that have it\n",
    "for df in datasets_with_consommation\n",
    "    df.consommation = replace.(df.consommation, \",\" => \".\")\n",
    "end\n",
    "\n",
    "# Convert 'cylindree' to float in all datasets\n",
    "for df in [train, valid, test]\n",
    "    df.cylindree = safe_parse_float.(df.cylindree)\n",
    "end\n",
    "\n",
    "# Convert 'consommation' to float in datasets with 'consommation'\n",
    "for df in datasets_with_consommation\n",
    "    df.consommation = safe_parse_float.(df.consommation)\n",
    "end\n",
    "\n",
    "# Drop missing values in all datasets\n",
    "for df in [train, valid, test]\n",
    "    dropmissing!(df)\n",
    "end\n",
    "\n",
    "# Encode 'boite' column in all datasets\n",
    "for df in [train, valid, test]\n",
    "    df.boite = ifelse.(df.boite .== \"automatique\", 1.0, 0.0)\n",
    "end\n",
    "\n",
    "# #cols_to_normalize = [:cylindree, :age, :nombre_cylindres]\n",
    "# train = normalize(train, cols_to_normalize)\n",
    "# valid = normalize(valid, cols_to_normalize)\n",
    "# test = normalize(test, cols_to_normalize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical columns\n",
    "categorical_cols = [:type, :transmission]\n",
    "\n",
    "# Collect unique levels from the training set\n",
    "levels_dict = Dict()\n",
    "for col in categorical_cols\n",
    "    levels_dict[col] = unique(train[!, col])\n",
    "end\n",
    "\n",
    "train = one_hot_encode(train, categorical_cols, levels_dict)\n",
    "valid = one_hot_encode(valid, categorical_cols, levels_dict)\n",
    "test = one_hot_encode(test, categorical_cols, levels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.consommation\n",
    "X_train = select(train, Not(:consommation))\n",
    "y_valid = valid.consommation\n",
    "X_valid = select(valid, Not(:consommation))\n",
    "X_test = deepcopy(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric feature indices\n",
    "feature_names = names(train)\n",
    "numeric_features = [ :cylindree, :nombre_cylindres, :age]\n",
    "numeric_indices = findall(x -> x in numeric_features, feature_names)\n",
    "\n",
    "means = mean(Matrix(X_train[:, numeric_features]), dims=1)\n",
    "stds = std(Matrix(X_train[:, numeric_features]), dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function standardizer(X, means, stds)\n",
    "    X = deepcopy(X)\n",
    "    for j in 1:size(X, 2)\n",
    "        if j in numeric_indices\n",
    "            X[:, j] = (X[:, j] .- means[j]) ./ stds[j]\n",
    "        end\n",
    "    end\n",
    "    return X\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = standardizer(Matrix(X_train), means, stds)\n",
    "X_valid = standardizer(Matrix(X_valid), means, stds)\n",
    "X_test = standardizer(Matrix(X_test), means, stds)\n",
    "\n",
    "y_train = Vector(y_train)\n",
    "y_valid = Vector(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression with cross-validation\n",
    "XtX = X_train' * X_train\n",
    "Xty = X_train' * y_train\n",
    "n_features = size(X_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_values = 10 .^ range(-5, stop=5, length=1000)\n",
    "best_rmse = Inf\n",
    "best_lambda = 0.0\n",
    "best_beta = nothing\n",
    "\n",
    "\n",
    "for Œª in lambda_values\n",
    "    beta = (XtX + Œª * I) \\ Xty\n",
    "    y_pred_valid = X_valid * beta\n",
    "    rmse = sqrt(mean((y_pred_valid - y_valid).^2))\n",
    "    println(\"Lambda: \", Œª, \" RMSE: \", rmse)\n",
    "    if rmse < best_rmse\n",
    "        best_rmse = rmse\n",
    "        best_lambda = Œª\n",
    "        best_beta = beta\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"Best Lambda: \", best_lambda)\n",
    "println(\"Best RMSE: \", best_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluation on validation set\n",
    "y_valid_pred = X_valid * best_beta\n",
    "rmse_valid = sqrt(mean((y_valid_pred - y_valid).^2))\n",
    "println(\"Validation RMSE: \", rmse_valid)\n",
    "\n",
    "# Predictions on test set\n",
    "y_test_pred = X_test * best_beta\n",
    "\n",
    "# Prepare submission DataFrame\n",
    "n_test = size(y_test_pred, 1)\n",
    "id = 1:n_test\n",
    "df_pred = DataFrame(id=id, consommation=y_test_pred)\n",
    "\n",
    "name = \"ridge\" * string(rmse_valid) * \".csv\"\n",
    "CSV.write(\"../submissions/\" * name, df_pred)\n",
    "println(\"Predictions exported successfully to \" * name*\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation par k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_k_folds = vcat(train, valid)\n",
    "y = data_k_folds.consommation\n",
    "X = select(data_k_folds, Not(:consommation))\n",
    "\n",
    "n = nrow(data_k_folds)\n",
    "k = 5  \n",
    "fold_size = n √∑ k\n",
    "\n",
    "indices = randperm(n)\n",
    "\n",
    "rms_scores = []\n",
    "\n",
    "for i in 0:(k-1)\n",
    "    test_indices = indices[(i*fold_size + 1):min((i+1)*fold_size, n)]\n",
    "    train_indices = setdiff(indices, test_indices)\n",
    "    \n",
    "    train_data = data_k_folds[train_indices, :]\n",
    "    test_data = data_k_folds[test_indices, :]\n",
    "    \n",
    "    model = lm(@formula(consommation ~ age + transmission_integrale + transmission_propulsion + transmission_traction + transmission_4x4 + cylindree), data_k_folds)\n",
    " \n",
    "    \n",
    "    valid_prediction = GLM.predict(model, test_data)\n",
    "    \n",
    "    mean_prediction = mean(skipmissing(valid_prediction))\n",
    "    valid_prediction = coalesce.(valid_prediction, mean_prediction)\n",
    "    \n",
    "    if any(ismissing, valid_prediction)\n",
    "        error(\"Skip les valeur missing\")\n",
    "    end\n",
    "    \n",
    "    v = max.(valid_prediction, 0) \n",
    "    \n",
    "    score = sqrt(mean((v - test_data.consommation).^2))\n",
    "    push!(rms_scores, score)\n",
    "end\n",
    "\n",
    "moyenne_rmse = mean(rms_scores)\n",
    "println(\"Moyenne RMSE : $moyenne_rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R√©gression par l'approche des composantes principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(1234) # For reproducibility\n",
    "\n",
    "# Split the data\n",
    "ntrain = round(Int, 0.8 * nrow(full_train))\n",
    "train_id = sample(1:nrow(full_train), ntrain; replace=false, ordered=true)\n",
    "valid_id = setdiff(1:nrow(full_train), train_id)\n",
    "\n",
    "train = full_train[train_id, :]\n",
    "valid = full_train[valid_id, :]\n",
    "\n",
    "# Data cleaning\n",
    "for col in [:cylindree, :consommation]\n",
    "    train[!, col] = replace.(train[!, col], \",\" => \".\")\n",
    "    valid[!, col] = replace.(valid[!, col], \",\" => \".\")\n",
    "    train[!, col] = safe_parse_float.(train[!, col])\n",
    "    valid[!, col] = safe_parse_float.(valid[!, col])\n",
    "end\n",
    "\n",
    "# Drop unnecessary columns\n",
    "train = select(train, Not([:type, :transmission, :boite]))\n",
    "valid = select(valid, Not([:type, :transmission, :boite]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TODO CONCLUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = Vector(train.consommation)\n",
    "X_train = Matrix(train[:, Not([:consommation])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centrer - r√©duire les donn√©es\n",
    "X_mean = mean(X_train; dims=1)\n",
    "X_stddev = std(X_train; dims=1, corrected=false)\n",
    "X_train_std = (X_train .- X_mean) ./ X_stddev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©composer en composantes principales\n",
    "pca_model = fit(PCA, X_train_std'; maxoutdim=8)\n",
    "\n",
    "# T = Z * V pour la matrice des composantes principales.\n",
    "Z_train = MultivariateStats.transform(pca_model, X_train_std')\n",
    "Z_train = Z_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model de regression sur les composantes principales\n",
    "model = lm(Z_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centrer - r√©duire les donn√©es de validation\n",
    "X_valid = Matrix(valid[:, Not([:consommation])])\n",
    "\n",
    "X_valid_std = (X_valid .- X_mean) ./ X_stddev\n",
    "\n",
    "Z_valid = MultivariateStats.transform(pca_model, X_valid_std')\n",
    "Z_valid = Z_valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_prediction = predict(model, Z_valid)\n",
    "\n",
    "mean_prediction = mean(skipmissing(valid_prediction))\n",
    "valid_prediction = coalesce.(valid_prediction, mean_prediction)\n",
    "\n",
    "mean_actual = mean(skipmissing(valid.consommation))\n",
    "actual_values = coalesce.(valid.consommation, mean_actual)\n",
    "\n",
    "rmse = sqrt(mean((valid_prediction - actual_values).^2))\n",
    "println(\"RMSE: \", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approche de pupuce \n",
    "# donn√©es ne sont pas standartis√©es\n",
    "y = train.consommation\n",
    "X = train[:, Not([:consommation])]\n",
    "\n",
    "X = Matrix(X)\n",
    "y = Vector(y)\n",
    "\n",
    "pca_model = fit(PCA, X, maxoutdim=8)\n",
    "\n",
    "# La PCA est appliqu√©e aux donn√©es non standardis√©es\n",
    "# Cela revient √† entra√Æner le mod√®le sur une version approximative \n",
    "#des donn√©es d'origine, ce qui peut r√©introduire la multicolin√©arit√© et annuler certains avantages de la PCA.\n",
    "Yte = predict(pca_model, X)\n",
    "Xr = reconstruct(pca_model, Yte)\n",
    "\n",
    "model = lm(Xr, y) \n",
    "consommation = valid.consommation\n",
    "select!(valid, Not(:consommation));\n",
    "X_valid = Matrix{Float64}(valid);\n",
    "valid_prediction = predict(model, X_valid)\n",
    "\n",
    "mean_prediction = mean(skipmissing(valid_prediction))\n",
    "\n",
    "valid_prediction = coalesce.(valid_prediction, mean_prediction)\n",
    "\n",
    "mean_actual = mean(skipmissing(consommation))\n",
    "actual_values = coalesce.(consommation, mean_actual)\n",
    "\n",
    "rmse = sqrt(mean((valid_prediction - actual_values).^2))\n",
    "println(\"RMSE: \", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xÃÑ = vec(mean(X, dims=1))\n",
    "\n",
    "Z = X .- xÃÑ'\n",
    "\n",
    "F = svd(Z)\n",
    "V = F.V\n",
    "U = F.U\n",
    "Œ≥ = F.S;\n",
    "\n",
    "cumvar = cumsum(Œ≥.^2)\n",
    "\n",
    "ratio = cumvar / cumvar[end]\n",
    "\n",
    "df = DataFrame(k = Int64[], Variance = Float64[])\n",
    "\n",
    "for k in 1:length(ratio)\n",
    "    push!(df, [k, ratio[k]])\n",
    "end\n",
    "\n",
    "Gadfly.plot(df, x=:k, y=:Variance, Geom.line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Impact des √©chelles dans les donn√©es\n",
    "Dans les donn√©es non standardis√©es, les variables explicatives ayant des valeurs plus grandes peuvent dominer la variance totale, ce qui oriente la PCA vers ces variables. Si ces variables sont fortement corr√©l√©es avec la variable cible (\n",
    "ùëå\n",
    "Y), alors le mod√®le peut accidentellement mieux capturer cette relation.\n",
    "En revanche, la standardisation neutralise les diff√©rences d'√©chelle, ce qui peut diluer l'effet des variables dominantes, m√™me si elles ont une forte corr√©lation avec \n",
    "ùëå\n",
    "Y.\n",
    "Explication potentielle :\n",
    "Dans votre jeu de donn√©es, il est possible qu'une ou plusieurs variables avec des √©chelles plus grandes soient pr√©dictives de \n",
    "ùëå\n",
    "Y, et la m√©thode non standardis√©e en profite directement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Sur-ajustement (Overfitting)\n",
    "La m√©thode non standardis√©e applique la PCA sur les donn√©es d'origine, mais utilise ensuite les donn√©es reconstruites (\n",
    "ùëã\n",
    "ùëü\n",
    "X \n",
    "r\n",
    "‚Äã\n",
    " ) pour l'entra√Ænement. Cela peut r√©introduire une grande partie des informations originales, y compris le bruit ou les corr√©lations spurielles, ce qui peut conduire √† un mod√®le sur-ajust√©.\n",
    "Si l'ensemble de validation est similaire √† l'ensemble d'entra√Ænement (par exemple, s'il provient de la m√™me distribution ou a des caract√©ristiques similaires), un sur-ajustement peut donner des RMSE artificiellement bas.\n",
    "Explication potentielle :\n",
    "Votre validation pourrait √™tre moins rigoureuse, et la m√©thode non standardis√©e exploite des relations non g√©n√©ralisables dans les donn√©es.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Le fait que la m√©thode non standardis√©e donne un meilleur RMSE peut s'expliquer par plusieurs raisons, mais cela ne signifie pas n√©cessairement qu'elle est meilleure ou qu'elle respecte les principes statistiques sous-jacents. Explorons pourquoi cela pourrait se produire :\n",
    "\n",
    "1. Impact des √©chelles dans les donn√©es\n",
    "Dans les donn√©es non standardis√©es, les variables explicatives ayant des valeurs plus grandes peuvent dominer la variance totale, ce qui oriente la PCA vers ces variables. Si ces variables sont fortement corr√©l√©es avec la variable cible (\n",
    "ùëå\n",
    "Y), alors le mod√®le peut accidentellement mieux capturer cette relation.\n",
    "En revanche, la standardisation neutralise les diff√©rences d'√©chelle, ce qui peut diluer l'effet des variables dominantes, m√™me si elles ont une forte corr√©lation avec \n",
    "ùëå\n",
    "Y.\n",
    "Explication potentielle :\n",
    "Dans votre jeu de donn√©es, il est possible qu'une ou plusieurs variables avec des √©chelles plus grandes soient pr√©dictives de \n",
    "ùëå\n",
    "Y, et la m√©thode non standardis√©e en profite directement.\n",
    "\n",
    "2. Sur-ajustement (Overfitting)\n",
    "La m√©thode non standardis√©e applique la PCA sur les donn√©es d'origine, mais utilise ensuite les donn√©es reconstruites (\n",
    "ùëã\n",
    "ùëü\n",
    "X \n",
    "r\n",
    "‚Äã\n",
    " ) pour l'entra√Ænement. Cela peut r√©introduire une grande partie des informations originales, y compris le bruit ou les corr√©lations spurielles, ce qui peut conduire √† un mod√®le sur-ajust√©.\n",
    "Si l'ensemble de validation est similaire √† l'ensemble d'entra√Ænement (par exemple, s'il provient de la m√™me distribution ou a des caract√©ristiques similaires), un sur-ajustement peut donner des RMSE artificiellement bas.\n",
    "Explication potentielle :\n",
    "Votre validation pourrait √™tre moins rigoureuse, et la m√©thode non standardis√©e exploite des relations non g√©n√©ralisables dans les donn√©es.\n",
    "\n",
    "3. Corr√©lation forte entre les variables\n",
    "Si les variables explicatives ont une forte corr√©lation intrins√®que, l'analyse en composantes principales standardis√©e peut r√©partir cette information sur plusieurs composantes. Cela r√©duit la capacit√© du mod√®le √† se concentrer sur des variables fortement corr√©l√©es avec \n",
    "ùëå\n",
    "Y.\n",
    "La m√©thode non standardis√©e, en revanche, conserve ces corr√©lations et peut donc mieux mod√©liser la relation entre \n",
    "ùëã\n",
    "X et \n",
    "ùëå\n",
    "Y.\n",
    "Explication potentielle :\n",
    "Les corr√©lations fortes dans vos donn√©es favorisent la m√©thode non standardis√©e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Effet des donn√©es reconstruites\n",
    "Dans la m√©thode non standardis√©e, vous utilisez des donn√©es reconstruites (\n",
    "ùëã\n",
    "ùëü\n",
    "X \n",
    "r\n",
    "‚Äã\n",
    " ), qui incluent une grande partie de l'information originale. Cela signifie que la r√©gression est moins influenc√©e par la r√©duction de dimension et plus proche de la r√©gression sur les donn√©es d'origine.\n",
    "En revanche, dans la m√©thode standardis√©e, seules les composantes principales sont utilis√©es, ce qui peut sacrifier une partie de l'information pour r√©duire la multicolin√©arit√© et am√©liorer la g√©n√©ralisation.\n",
    "Explication potentielle :\n",
    "L'utilisation des donn√©es reconstruites dans la m√©thode non standardis√©e maintient plus d'information, ce qui peut donner un RMSE plus faible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Probl√®me avec la PCA standardis√©e\n",
    "Si la standardisation n'est pas appropri√©e (par exemple, si certaines variables explicatives sont presque constantes ou si elles sont d√©j√† sur des √©chelles comparables), alors l'analyse en composantes principales standardis√©e peut ne pas capturer efficacement les directions principales de la variance.\n",
    "Cela peut entra√Æner une perte d'information utile, ce qui affecte les performances du mod√®le.\n",
    "Explication potentielle :\n",
    "Les variables de votre jeu de donn√©es n'ont peut-√™tre pas besoin d'√™tre standardis√©es ou la standardisation introduit un biais."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.5",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
